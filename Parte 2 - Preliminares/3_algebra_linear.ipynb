{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Álgebra Linear\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notações para valores escalares:\n",
    "\n",
    "$$ x, y, z \\in \\mathbb{R} $$\n",
    "\n",
    "Escalares são implementados em tensores que contém apenas um elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n",
      "tensor(-1.)\n",
      "tensor(12.)\n",
      "tensor(0.7500)\n",
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(4.0)\n",
    "\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x ** y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Como escrever vetores e matrizes em Markdown](https://definirtec.com/latex-criando-uma-matriz-como-faze-lo/)\n",
    "\n",
    "Vetores são como listas de escalares. Os valores da lista são os elementos, entradas. NO exemplo abaixo, $a, b$ e $c$ seriam os escalares, entradas, do vetor:\n",
    "\n",
    "$$ v = \n",
    "\\begin{bmatrix} \n",
    "a \\\\\n",
    "b \\\\\n",
    "c \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetores são tensores de primeira ordem, e é importante lembrar que a indexação na maioria das linguagens de programação começa em zero enquanto em conteúdos teóricos é mais comum começar em um. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja o vetor:\n",
    "\n",
    "$$ v = \n",
    "\\begin{bmatrix} \n",
    "x_1\\\\\n",
    "\\vdots \\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Onde $x_1$ representa o primeiro elemento e $x_n$ representa o enésimo do vetor v. Dizemos que $x \\in \\mathbb{R}^n$, ou seja, $x$ tem $n$ dimensões, $n$ entradas. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse valor é chamado de *dimensionalidade* do vetor. Para saber a *dimensionalidade* de um vetor no Python, basta a função abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar a forma, temos a função abaixo. Ela indica em uma tupla o comprimeto de cada eixo do vetor. No nosso exemplo, no qual o vetor só tem um eixo com 10 elementos, o valor resultante é 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para não ficar confuso, geralmente utiliza-se de *order* (Ordem) para indicar quantos eixos o vetor pode ter e *dimensionalidade* fica para referenciar a quantidade de elementos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrizes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrizes são vetores de segunda ordem, ou seja, possuem dois eixos. Podemos representar uma matriz a partir de um vetor utilizando a função reshape. \n",
    "\n",
    "Veja abaixo o código do reshape e a representação de uma matriz $A \\in \\mathbb{R}^{n * m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ A =\n",
    "\n",
    "\\begin{pmatrix*}[r] \n",
    "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n",
    "\\end{pmatrix*} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(18).reshape(3, 6)\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer a **transposta** de uma matriz, que é tratar as linhas como colunas e as colunas como linhas, basta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 12],\n",
      "        [ 1,  7, 13],\n",
      "        [ 2,  8, 14],\n",
      "        [ 3,  9, 15],\n",
      "        [ 4, 10, 16],\n",
      "        [ 5, 11, 17]])\n"
     ]
    }
   ],
   "source": [
    "print(A.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrizes Simétricas** são aquelas que são iguais as suas transpostas. Ou seja $A = A^{T}$. Veja como verificar isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "print(A == A.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de vetores de primeira e segunda ordem (matrizes), será necessário trabalhar com vetores de odens maiores. Um exemplo de dado que utiliza um vetor de terceira ordem são os dados relativos a imagens:\n",
    "- Altura;\n",
    "- Largura;\n",
    "- Channel - que guarda a intensidade de cada cor (RGB - Red, Green e Blue);\n",
    "\n",
    "Esse é só um exemplo de como podemos trabalhar com imagens e a motivação por de trás da necessidade de tensores de ordens maiores. Veja abaixo um tensor de 3º ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É como se fosse um paralelepipedo com cada camada sendo uma matriz, e tendo no total, 2 dessas camadas de matriz. Como se fosse uma fila de matrizes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades Básicas de Aritmética de Tensores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos algumas operações básicas com tensores:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "A + B =  tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n",
      "2 * A =  tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Copia A em B, alocando uma nova memória; \n",
    "\n",
    "print(A)\n",
    "\n",
    "print(\"A + B = \", A + B)\n",
    "print(\"2 * A = \", 2*A)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard Product:\n",
    "\n",
    "Multiplica o elemento $a_{ijk...}$ pelo elemento $b_{ijk...}$ e forma um novo tensor. É chamado de Hadamard Product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A * B =  tensor([[ 0.,  1.,  4.],\n",
      "        [ 9., 16., 25.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"A * B = \", A * B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operações de Tensor com Escalar:\n",
    "\n",
    "As operações são realizadas somando, subtraindo, dividindo ou multiplicando o escapar por todo elemento do tensor. Veja abaixo um exemplo da multipliação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "C * 100 =  tensor([[[   0,  100,  200,  300],\n",
      "         [ 400,  500,  600,  700],\n",
      "         [ 800,  900, 1000, 1100]],\n",
      "\n",
      "        [[1200, 1300, 1400, 1500],\n",
      "         [1600, 1700, 1800, 1900],\n",
      "         [2000, 2100, 2200, 2300]]])\n"
     ]
    }
   ],
   "source": [
    "C = torch.arange(24).reshape(2, 3, 4)\n",
    "print(C)\n",
    "\n",
    "print(\"C * 100 = \", C * 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction - Redução:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular a soma de todos os elementos de um vetor de dimensionalidade $n$:\n",
    "\n",
    "$$\\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "Para obter esse valor, tem uma função pronta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
      "        29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
      "        43., 44., 45., 46., 47., 48., 49., 50.], dtype=torch.float64)\n",
      "tensor(1275., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(50, dtype=float)\n",
    "X[:] = X + 1\n",
    "print(X)\n",
    "print(X.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um tensor de forma arbitrária, a soma devolve a soma de todos os elementos do tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9., 10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15., 16., 17., 18.],\n",
      "         [19., 20., 21., 22., 23., 24.]],\n",
      "\n",
      "        [[25., 26., 27., 28., 29., 30.],\n",
      "         [31., 32., 33., 34., 35., 36.]]], dtype=torch.float64)\n",
      "tensor(666., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Y = torch.arange(36, dtype=float).reshape(3, 2, 6)\n",
    "Y[:] = Y + 1\n",
    "print(Y)\n",
    "\n",
    "print(Y.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível também especificar em quais eixos se deseja realizar a soma, para isso, basta especificar por meio do índice do eixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 6])\n",
      "tensor([[39., 42., 45., 48., 51., 54.],\n",
      "        [57., 60., 63., 66., 69., 72.]], dtype=torch.float64)\n",
      "tensor([[ 8., 10., 12., 14., 16., 18.],\n",
      "        [32., 34., 36., 38., 40., 42.],\n",
      "        [56., 58., 60., 62., 64., 66.]], dtype=torch.float64)\n",
      "tensor([[ 21.,  57.],\n",
      "        [ 93., 129.],\n",
      "        [165., 201.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.sum(axis=0)) # Ao longo do eixo z; (3)\n",
    "print(Y.sum(axis=1)) # Ao longo do eixo y; (2)\n",
    "print(Y.sum(axis=2)) # Ao longo do eixo x; (6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer a soma em dois eixos ao mesmo instante, podemos fazer o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 96., 102., 108., 114., 120., 126.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y.sum(axis=[0, 1])) # Soma os valores no eixo z e y;\n",
    "# 1+ 13 + 25 + 7 + 19 + 31 = 96\n",
    "# E assim por diante;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum() == Y.sum(axis=[0, 1, 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a média usa-se a seguinte função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.5000, dtype=torch.float64)\n",
      "tensor(18.5000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y.mean()) # Não funiona se os valores forem inteiros pelo jeito;\n",
    "print(Y.sum() / Y.numel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível fazer a média em torno de um eixo, o especificando da mesma forma feira anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5000,  9.5000],\n",
      "        [15.5000, 21.5000],\n",
      "        [27.5000, 33.5000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(Y.mean(axis=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Reduction Sum - Soma s/ Redução"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para calcular a soma sem mexer nas dimensões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.)\n",
      "torch.Size([])\n",
      "tensor([[ 3.],\n",
      "        [12.]])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "soma_A = A.sum()\n",
    "print(soma_A)\n",
    "print(soma_A.shape)\n",
    "soma_A_sem_reducao = A.sum(axis=1, keepdims=True)\n",
    "print(soma_A_sem_reducao)\n",
    "print(soma_A_sem_reducao.shape)\n",
    "\n",
    "# Comparando as sáidas, vemos a 'soma_A_sem_reducao' manteve o shape;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então dividir A por soma_A_sem_reducao por *broadcasting* (veja resumo 1 da parte 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3333, 0.6667],\n",
      "        [0.2500, 0.3333, 0.4167]])\n",
      "tensor([[0.0000, 0.0667, 0.1333],\n",
      "        [0.2000, 0.2667, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(A / soma_A_sem_reducao)\n",
    "print(A / soma_A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há também uma função para a soma acumulativa em torno de algum eixo. Veja abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 5., 7.]])\n",
      "tensor([[ 0.,  1.,  3.],\n",
      "        [ 3.,  7., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print(A.cumsum(axis=0))\n",
    "print(A.cumsum(axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Products - Produto Escalar "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product é o **Produto Escalar** entre dois vetores, e é calculado da seguinte forma:\n",
    "\n",
    "$$ x = \n",
    "\\begin{bmatrix} \n",
    "x_1\\\\\n",
    "\\vdots \\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix} \n",
    "\n",
    "y =\n",
    "\\begin{bmatrix} \n",
    "y_1\\\\\n",
    "\\vdots \\\\\n",
    "y_n\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$\n",
    "x \\cdot y\\ = \\sum_{i=1}^{n} x_i * y_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, resulta em um valor escalar. Para fazer essa operação em Python, usamos a seguinte função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y) # Também é possível fazer assim. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse tipo de operação é bem útil por exemplo quando formos tratar de vetor de pesos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Vector Produts - Produto de Matriz por Vetor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver matrizes como uma lista de vetores. Onde cada linha é um vetor (*row vector*). Veja a notação abaixo, considerando uma matriz $A \\in \\mathbb{R}^{n*m}$:\n",
    "\n",
    "$$ A = \n",
    "\\begin{pmatrix*}[r] \n",
    "a_{1}^T \\\\\n",
    "a_{2}^t \\\\\n",
    "\\vdots  \\\\\n",
    "a_{m}^T \\\\\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "Onde $a_i^T$ representa um *row vector* da linha $i$ e $\\in \\mathbb{R}^n$. Colocamos o $T$ de *Transposta* pois costumamos representar vetores de forma vertical. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dess forma, sendo $x \\in \\mathbb{R}^n$ um vetor, a multiplicação de $A$ por $x$ é dada por:\n",
    "\n",
    "$$ Ax = \n",
    "\\begin{pmatrix*}[r] \n",
    "a_{1}^T \\cdot x \\\\\n",
    "a_{2}^t \\cdot x\\\\\n",
    "\\vdots  \\\\\n",
    "a_{m}^T \\cdot x\\\\\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "Ou seja, fazemos o **dot product** de todo vetor linha pelo vetor $x$. No final, teremos $Ax = y \\in \\mathbb{R}^m$. Ou seja, podemos ver a matriz $A$ como uma **transformação** que projeta o vetor $x$ de $\\mathbb{R}^n$ para $\\mathbb{R}^m$.\n",
    "\n",
    "Esses produtos são muito importantes e terão várias aplicações posteriomente. Para fazer em Python, veja o código abaixo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "torch.Size([2, 3])\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n",
      "tensor([ 5., 14.])\n",
      "tensor([ 5., 14.])\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(A.shape)\n",
    "\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(torch.mv(A, x))\n",
    "print(A @ x) # É a mesma operação acima;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produto de Matriz por Matriz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere a representação a seguir das matrizes $A \\in \\mathbb{R}^{n*k}$ e $B \\in \\mathbb{R}^{k*m}$:\n",
    "\n",
    "$$ A =\n",
    "\\begin{pmatrix*}[r] \n",
    "a_{11} & a_{12} & \\dots & a_{1k} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\dots & a_{nk} \\\\\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "$$ B =\n",
    "\\begin{pmatrix*}[r] \n",
    "b_{11} & b_{12} & \\dots & b_{1m} \\\\\n",
    "b_{21} & b_{22} & \\dots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k1} & b_{k2} & \\dots & b_{km} \\\\\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "Seja $a_i^T \\in \\mathbb{R}^k$ os vetores linha transpostos de $A$ e $b_i \\in \\mathbb{R}^k$ os vetores coluna de $B$.\n",
    "\n",
    "$$ A = \n",
    "\\begin{pmatrix*}[r] \n",
    "a_{1}^T \\\\\n",
    "a_{2}^t \\\\\n",
    "\\vdots  \\\\\n",
    "a_{n}^T \\\\\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "$$ B = \n",
    "\\begin{pmatrix*}[r] \n",
    "b_1 & b_2 & b_3 & \\dots & b_{m}\n",
    "\\end{pmatrix*} $$\n",
    "\n",
    "Assim, podemos escrever o produto $AB$ da seguinte forma:\n",
    "\n",
    "$$\n",
    "C = AB = \n",
    "\\begin{pmatrix*}[r] \n",
    "a_{1}^T \\\\\n",
    "a_{2}^t \\\\\n",
    "\\vdots  \\\\\n",
    "a_{n}^T \\\\\n",
    "\\end{pmatrix*} \n",
    "\n",
    "\\begin{pmatrix*}[r] \n",
    "b_1 & b_2 & \\dots & b_{m}\n",
    "\\end{pmatrix*} \n",
    "\n",
    "= \n",
    "\\begin{pmatrix*}[r]\n",
    "a_1^Tb_1 & a_1^Tb_2  & \\dots & a_1^Tb_m \\\\\n",
    "a_2^Tb_1 & a_2^Tb_2  & \\dots & a_2^Tb_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_n^Tb_1 & a_n^Tb_2  & \\dots & a_n^Tb_m \\\\\n",
    "\\end{pmatrix*}\n",
    "$$\n",
    "\n",
    "Ou seja, um montão de produtos escalares. Para fazer isso em Python, veja o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 3.,  3.,  3.,  3.],\n",
      "        [12., 12., 12., 12.]])\n",
      "tensor([[ 3.,  3.,  3.,  3.],\n",
      "        [12., 12., 12., 12.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x4 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mmm(A, B))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(A \u001b[39m@\u001b[39m B)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(B \u001b[39m@\u001b[39;49m A) \u001b[39m# Importante destacar que não é uma operação simétrica\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# e depende das dimensões da linha e colunas. \u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x4 and 2x3)"
     ]
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "print(B)\n",
    "\n",
    "print(torch.mm(A, B))\n",
    "print(A @ B)\n",
    "print(B @ A) # Importante destacar que não é uma operação simétrica\n",
    "# e depende das dimensões da linha e colunas. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $A \\in \\mathbb{R}^{x*y}$ e $B \\in \\mathbb{R}^{z*w}$:\n",
    "- Se $y = z$ então $\\exists AB$;\n",
    "- Se $x = w$ então $\\exists BA$;\n",
    "- Não é sempre que $AB = BA$; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As normas são, de forma simplificada, o módulo de um vetor. É sua distância até a origem, representa o quão \"grande\" é o vetor. Enfim, a interpretação pode variar um pouco, mas é importante considerar a definição matemática.\n",
    "\n",
    "A **norma** de um vetor, representada por essa notação $\\|\\mathbf{.}\\| $ tem as seguintes propriedades:\n",
    "- $ \\|\\mathbf{\\alpha x}\\|$ = $ |\\alpha|\\|\\mathbf{x}\\|$\n",
    "- $ \\|\\mathbf{x + y}\\| \\leq \\|\\mathbf{x}\\|$ + $ \\|\\mathbf{y}\\|$\n",
    "- $ \\|\\mathbf{x}\\| \\gt 0$ se $x \\neq 0$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos diferentes definições de **normas** e formas de se calcular a mesma. Uma delas é a **Euclidean norm** que é a raiz da soma dos quadrados de todos os elementos do vetor (Considere $x \\in \\mathbb{R}^n $):\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_2  = \\sqrt{\\sum_{i=1}^{n} x_i^2}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular me Python, temos o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u) # 3^2 + (-4)^2 = 25, 25^{1/2} = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos também a norma conhecida como **Manhattan distance** que é a soma dos módulos dos elementos do vetor:\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_1  = \\sum_{i=1}^{n} |x_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum() # |3| + |-4| = 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E uma norma mais geral é dada pela seguinte fórmula:\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_p  = (\\sum_{i=1}^{n} |x_i|^p)^\\frac{1}{p}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para matrizes, a ideia é um pouco mais complexa. Uma das normas, chamada de **Frobenius norm**, é dada por:\n",
    "\n",
    "$$ \\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} x_{ij}^2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))\n",
    "# É uma matriz de 4 X 9, cheias de 1, ou seja, 36 uns;\n",
    "# Raiz de 36 dá 6;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
